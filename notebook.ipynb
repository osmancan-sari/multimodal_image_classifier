{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e14c7f05-e5ea-46f5-b4fd-936f8cafadf2",
   "metadata": {},
   "source": [
    "### Steps of the Project:\n",
    "- Import libraries\n",
    "- Extract the data and form dataframes\n",
    "- Try three models\n",
    "- Hyperparameter tuning using Optuna on best performing model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5947ecdc-bf18-4942-9b57-d49b0d06ea70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training DataFrame Shape: (40000, 6)\n",
      "   idx                                     resnet_feature  \\\n",
      "0    0  [0.882933, 1.7576884, 0.037083052, 0.080605745...   \n",
      "1    1  [1.856296, 0.08484737, 1.9896222, 0.37220904, ...   \n",
      "2    2  [1.0885917, 0.9853252, 1.2479072, 0.85630983, ...   \n",
      "3    3  [0.46679923, 0.59049946, 1.0448841, 0.43161052...   \n",
      "4    4  [1.6842757, 4.21578, 0.45899037, 0.30567297, 0...   \n",
      "\n",
      "                                         vit_feature  \\\n",
      "0  [0.39251244, -0.7359995, -0.40713686, 0.384909...   \n",
      "1  [0.536756, 0.20046183, 0.53705513, -1.4450091,...   \n",
      "2  [0.18865782, 0.40380907, 0.08730512, 1.3314861...   \n",
      "3  [0.9024208, 1.004203, 0.62072945, 0.40194172, ...   \n",
      "4  [-1.0811086, 0.018160842, 0.40104544, -0.41978...   \n",
      "\n",
      "                                        clip_feature  \\\n",
      "0  [0.36834767, -0.0048528686, -0.54004794, -0.42...   \n",
      "1  [0.21078074, -0.23845242, -0.23498438, -0.0415...   \n",
      "2  [0.23849042, -0.17168012, -0.33907497, -0.1153...   \n",
      "3  [0.27193326, 0.05075098, -0.2544187, -0.014354...   \n",
      "4  [-0.024229065, -0.15305626, -0.37416303, 0.284...   \n",
      "\n",
      "                                        dino_feature  label  \n",
      "0  [-1.5148295, 0.5156541, -1.34948, 0.32569966, ...      1  \n",
      "1  [0.60461986, 0.6316875, -2.3706322, -4.3395443...      3  \n",
      "2  [0.35535327, -0.5768385, 0.9632951, 0.421463, ...      8  \n",
      "3  [-1.7897346, -0.89510065, -0.87318826, -0.1660...      7  \n",
      "4  [-1.3486544, 0.41463324, -0.054911207, 1.67490...      9  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as r\n",
    "import optuna\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "r.seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# Load train_feats.npy and train_labels.csv\n",
    "train_feats = np.load('train_feats.npy', allow_pickle=True).item()\n",
    "train_labels = pd.read_csv('train_labels.csv')\n",
    "\n",
    "# Prepare the training DataFrame\n",
    "final_train_df = pd.DataFrame({\n",
    "    'idx': train_feats['idx'],\n",
    "    'resnet_feature': list(train_feats['resnet_feature']),\n",
    "    'vit_feature': list(train_feats['vit_feature']),\n",
    "    'clip_feature': list(train_feats['clip_feature']),\n",
    "    'dino_feature': list(train_feats['dino_feature']),\n",
    "    'label': train_labels['label']  # Add ground truth labels\n",
    "})\n",
    "\n",
    "# Check the prepared training DataFrame\n",
    "print(\"Final Training DataFrame Shape:\", final_train_df.shape)\n",
    "print(final_train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a780e578-5a29-41bb-bf03-0052c73eb88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation-Test DataFrame Shape: (20000, 5)\n",
      "   idx                                     resnet_feature  \\\n",
      "0    0  [1.8273036, 0.63109094, 0.23980471, 1.7095062,...   \n",
      "1    1  [0.90152156, 0.46560124, 0.4799108, 1.1381094,...   \n",
      "2    2  [1.0099123, 1.7757976, 0.07629461, 0.5221749, ...   \n",
      "3    3  [0.010587645, 3.5005102, 0.01937968, 0.7267904...   \n",
      "4    4  [0.47045803, 0.45250612, 0.115028925, 0.298091...   \n",
      "\n",
      "                                         vit_feature  \\\n",
      "0  [1.3817077, 0.87259007, 0.04867079, 1.398396, ...   \n",
      "1  [-0.35124308, -0.19813143, -0.54856735, 0.1956...   \n",
      "2  [1.0760875, 0.682714, 0.40147147, 0.58281916, ...   \n",
      "3  [0.3766103, 0.37389246, 0.256509, 1.0673461, -...   \n",
      "4  [0.9422639, 0.74399835, -0.21973425, 0.1927124...   \n",
      "\n",
      "                                        clip_feature  \\\n",
      "0  [0.29609913, 0.20632589, -0.30062598, -0.34170...   \n",
      "1  [-0.020611845, -0.19302031, 0.122885466, -0.40...   \n",
      "2  [0.47820017, -0.047153465, 0.107344255, 0.1043...   \n",
      "3  [0.18480419, -0.14510047, -0.19404492, -0.3681...   \n",
      "4  [0.2129071, -0.23783478, -0.35091218, 0.006192...   \n",
      "\n",
      "                                        dino_feature  \n",
      "0  [1.1167926, -4.3160276, 0.36962438, -2.5382924...  \n",
      "1  [-1.394359, 1.7436061, -0.686435, 0.6915347, 1...  \n",
      "2  [1.6119182, 1.5726092, 1.6961548, -3.275578, 1...  \n",
      "3  [2.3936064, 0.59399194, 0.00481878, -1.4603308...  \n",
      "4  [2.5054302, 0.32808575, -0.48522863, -1.425354...  \n"
     ]
    }
   ],
   "source": [
    "# Load valtest_feats.npy\n",
    "valtest_feats = np.load('valtest_feats.npy', allow_pickle=True).item()\n",
    "\n",
    "# Prepare the validation-test DataFrame\n",
    "valtest_df = pd.DataFrame({\n",
    "    'idx': valtest_feats['idx'],\n",
    "    'resnet_feature': list(valtest_feats['resnet_feature']),\n",
    "    'vit_feature': list(valtest_feats['vit_feature']),\n",
    "    'clip_feature': list(valtest_feats['clip_feature']),\n",
    "    'dino_feature': list(valtest_feats['dino_feature'])\n",
    "})\n",
    "\n",
    "# Check the validation-test DataFrame\n",
    "print(\"Validation-Test DataFrame Shape:\", valtest_df.shape)\n",
    "print(valtest_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "011dc637-57f5-43a7-b731-c7ff541d68b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx               0\n",
      "resnet_feature    0\n",
      "vit_feature       0\n",
      "clip_feature      0\n",
      "dino_feature      0\n",
      "label             0\n",
      "dtype: int64\n",
      "idx               0\n",
      "resnet_feature    0\n",
      "vit_feature       0\n",
      "clip_feature      0\n",
      "dino_feature      0\n",
      "dtype: int64\n",
      "ResNet Shape: (40000, 512)\n",
      "ViT Shape: (40000, 768)\n",
      "CLIP Shape: (40000, 512)\n",
      "DiNO Shape: (40000, 768)\n"
     ]
    }
   ],
   "source": [
    "print(final_train_df.isnull().sum())  # Check for missing values\n",
    "print(valtest_df.isnull().sum())\n",
    "print(\"ResNet Shape:\", np.shape(train_feats['resnet_feature']))\n",
    "print(\"ViT Shape:\", np.shape(train_feats['vit_feature']))\n",
    "print(\"CLIP Shape:\", np.shape(train_feats['clip_feature']))\n",
    "print(\"DiNO Shape:\", np.shape(train_feats['dino_feature']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b140edd0-e20e-4876-952d-a1dd186c2b7b",
   "metadata": {},
   "source": [
    "### MODEL 1 : XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "78543d9b-064e-4cd9-96e1-a5874bde2434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform 5-fold cross-validation for XGBoost\n",
    "def cross_validate_xgboost(train_data, model_params):\n",
    "    \"\"\"\n",
    "    Perform 5-fold cross-validation using XGBoost.\n",
    "\n",
    "    Parameters:\n",
    "    - train_data: pandas DataFrame, contains features and labels\n",
    "    - model_params: dict, parameters for the XGBoost model\n",
    "\n",
    "    Returns:\n",
    "    - average_f1: float, average macro F1 score across folds\n",
    "    - predictions: list, all predictions from 5 folds\n",
    "    \"\"\"\n",
    "    X = np.hstack([\n",
    "        np.stack(train_data['resnet_feature']),\n",
    "        np.stack(train_data['vit_feature']),\n",
    "        np.stack(train_data['clip_feature']),\n",
    "        np.stack(train_data['dino_feature'])\n",
    "    ])\n",
    "    y = train_data['label']\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    f1_scores = []\n",
    "    all_predictions = []\n",
    "\n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        # Train XGBoost model\n",
    "        model = xgb.XGBClassifier(\n",
    "            **model_params,\n",
    "            objective='multi:softmax',\n",
    "            num_class=len(np.unique(y)),\n",
    "            random_state=42,\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_val)\n",
    "\n",
    "        # Calculate F1 score\n",
    "        f1 = f1_score(y_val, predictions, average='macro')\n",
    "        f1_scores.append(f1)\n",
    "        all_predictions.extend(predictions)\n",
    "\n",
    "    average_f1 = np.mean(f1_scores)\n",
    "    return average_f1, all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f4839a21-f962-43b5-9ec5-0a9eff64eb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score for XGBoost: 0.9814\n"
     ]
    }
   ],
   "source": [
    " # XGBoost parameters\n",
    "xgb_params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8\n",
    "}\n",
    "\n",
    "# Perform 5-fold CV for XGBoost\n",
    "avg_f1_xgb, preds_xgb = cross_validate_xgboost(final_train_df, xgb_params)\n",
    "print(f\"Average F1 Score for XGBoost: {avg_f1_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406a8b2c-4305-4d75-ae04-b2de2b1a02c8",
   "metadata": {},
   "source": [
    "### MODEL 2: LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6da6555b-f705-4de3-923e-20c99db3f915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform 5-fold cross-validation for LightGBM\n",
    "def cross_validate_lightgbm(train_data, model_params):\n",
    "    \"\"\"\n",
    "    Perform 5-fold cross-validation using LightGBM.\n",
    "\n",
    "    Parameters:\n",
    "    - train_data: pandas DataFrame, contains features and labels\n",
    "    - model_params: dict, parameters for the LightGBM model\n",
    "\n",
    "    Returns:\n",
    "    - average_f1: float, average macro F1 score across folds\n",
    "    - predictions: list, all predictions from 5 folds\n",
    "    \"\"\"\n",
    "    X = np.hstack([\n",
    "        np.stack(train_data['resnet_feature']),\n",
    "        np.stack(train_data['vit_feature']),\n",
    "        np.stack(train_data['clip_feature']),\n",
    "        np.stack(train_data['dino_feature'])\n",
    "    ])\n",
    "    y = train_data['label']\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    f1_scores = []\n",
    "    all_predictions = []\n",
    "\n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        # Train LightGBM model\n",
    "        train_data_lgb = lgb.Dataset(X_train, label=y_train)\n",
    "        val_data_lgb = lgb.Dataset(X_val, label=y_val, reference=train_data_lgb)\n",
    "\n",
    "        model = lgb.train(\n",
    "            model_params,\n",
    "            train_data_lgb,\n",
    "            valid_sets=[val_data_lgb],\n",
    "        )\n",
    "\n",
    "        # Predict on validation set\n",
    "        predictions = model.predict(X_val)\n",
    "        predictions = np.argmax(predictions, axis=1)  # Convert probabilities to class labels\n",
    "\n",
    "        # Calculate F1 score\n",
    "        f1 = f1_score(y_val, predictions, average='macro')\n",
    "        f1_scores.append(f1)\n",
    "        all_predictions.extend(predictions)\n",
    "\n",
    "    average_f1 = np.mean(f1_scores)\n",
    "    return average_f1, all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5c76d8c1-3c70-465e-8633-84f281742a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.272470 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 652800\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 2560\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.280833 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 652800\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 2560\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.280919 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 652800\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 2560\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.277852 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 652800\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 2560\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.278733 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 652800\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 2560\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "Average F1 Score for LightGBM: 0.9842\n"
     ]
    }
   ],
   "source": [
    " # Load prepared training data\n",
    "train_feats = np.load('train_feats.npy', allow_pickle=True).item()\n",
    "train_labels = pd.read_csv('train_labels.csv')\n",
    "\n",
    "final_train_df = pd.DataFrame({\n",
    "    'idx': train_feats['idx'],\n",
    "    'resnet_feature': list(train_feats['resnet_feature']),\n",
    "    'vit_feature': list(train_feats['vit_feature']),\n",
    "    'clip_feature': list(train_feats['clip_feature']),\n",
    "    'dino_feature': list(train_feats['dino_feature']),\n",
    "    'label': train_labels['label']  # Add ground truth labels\n",
    "})\n",
    "\n",
    "# LightGBM parameters\n",
    "lgb_params = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': len(np.unique(final_train_df['label'])),\n",
    "    'metric': 'multi_logloss',\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': -1,\n",
    "    'num_leaves': 31,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'seed': 1\n",
    "}\n",
    "\n",
    "# Perform 5-fold CV for LightGBM\n",
    "avg_f1_lgb, preds_lgb = cross_validate_lightgbm(final_train_df, lgb_params)\n",
    "print(f\"Average F1 Score for LightGBM: {avg_f1_lgb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec93a64-1b05-4e80-b79b-681b672b9469",
   "metadata": {},
   "source": [
    "### MODEL 3: Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32b2408a-d598-499d-9ffb-f2c8c555039a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-25 14:13:38,611] A new study created in memory with name: no-name-92874662-d1b9-4c4e-85ee-7e0ac24bbb13\n",
      "[I 2024-12-25 14:14:34,057] Trial 0 finished with value: 0.9876362879143386 and parameters: {'num_layers': 3, 'hidden_size': 214, 'learning_rate': 0.00043366667467870074, 'batch_size': 128}. Best is trial 0 with value: 0.9876362879143386.\n",
      "[I 2024-12-25 14:25:26,613] Trial 1 finished with value: 0.9798421563461792 and parameters: {'num_layers': 3, 'hidden_size': 445, 'learning_rate': 0.001706066207505558, 'batch_size': 16}. Best is trial 0 with value: 0.9876362879143386.\n",
      "[I 2024-12-25 14:26:29,155] Trial 2 finished with value: 0.9842531764130037 and parameters: {'num_layers': 2, 'hidden_size': 479, 'learning_rate': 0.0006210374819609473, 'batch_size': 128}. Best is trial 0 with value: 0.9876362879143386.\n",
      "[I 2024-12-25 14:27:45,561] Trial 3 finished with value: 0.9694701865591651 and parameters: {'num_layers': 2, 'hidden_size': 219, 'learning_rate': 0.005630892954221425, 'batch_size': 64}. Best is trial 0 with value: 0.9876362879143386.\n",
      "[I 2024-12-25 14:28:42,962] Trial 4 finished with value: 0.9881407349153655 and parameters: {'num_layers': 2, 'hidden_size': 428, 'learning_rate': 0.0002947616994136992, 'batch_size': 128}. Best is trial 4 with value: 0.9881407349153655.\n",
      "[I 2024-12-25 14:45:46,441] Trial 5 finished with value: 0.9862705752489905 and parameters: {'num_layers': 3, 'hidden_size': 323, 'learning_rate': 0.00013301575924937566, 'batch_size': 16}. Best is trial 4 with value: 0.9881407349153655.\n",
      "[I 2024-12-25 14:46:42,420] Trial 6 finished with value: 0.9831627638383053 and parameters: {'num_layers': 1, 'hidden_size': 286, 'learning_rate': 0.0017807238123728104, 'batch_size': 128}. Best is trial 4 with value: 0.9881407349153655.\n",
      "[I 2024-12-25 14:51:34,569] Trial 7 finished with value: 0.9858708977614435 and parameters: {'num_layers': 2, 'hidden_size': 209, 'learning_rate': 0.0002628430279772363, 'batch_size': 16}. Best is trial 4 with value: 0.9881407349153655.\n",
      "[I 2024-12-25 14:52:40,215] Trial 8 finished with value: 0.9855062514076302 and parameters: {'num_layers': 2, 'hidden_size': 93, 'learning_rate': 0.0008120179983397094, 'batch_size': 32}. Best is trial 4 with value: 0.9881407349153655.\n",
      "[I 2024-12-25 15:01:07,824] Trial 9 finished with value: 0.9865110103061033 and parameters: {'num_layers': 3, 'hidden_size': 418, 'learning_rate': 0.00022491752600465809, 'batch_size': 32}. Best is trial 4 with value: 0.9881407349153655.\n",
      "[I 2024-12-25 15:03:10,925] Trial 10 finished with value: 0.9897561416699668 and parameters: {'num_layers': 1, 'hidden_size': 379, 'learning_rate': 0.00010118219003112448, 'batch_size': 64}. Best is trial 10 with value: 0.9897561416699668.\n",
      "[I 2024-12-25 15:05:05,441] Trial 11 finished with value: 0.9882617746528384 and parameters: {'num_layers': 1, 'hidden_size': 372, 'learning_rate': 0.00011006457718712734, 'batch_size': 64}. Best is trial 10 with value: 0.9897561416699668.\n",
      "[I 2024-12-25 15:06:52,069] Trial 12 finished with value: 0.9900113179487751 and parameters: {'num_layers': 1, 'hidden_size': 349, 'learning_rate': 0.00013466727321237925, 'batch_size': 64}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 15:08:43,451] Trial 13 finished with value: 0.987634144611957 and parameters: {'num_layers': 1, 'hidden_size': 345, 'learning_rate': 0.00010143372761357458, 'batch_size': 64}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 15:10:05,760] Trial 14 finished with value: 0.9886400608148673 and parameters: {'num_layers': 1, 'hidden_size': 275, 'learning_rate': 0.00018028605116365108, 'batch_size': 64}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 15:12:13,813] Trial 15 finished with value: 0.9849003341856338 and parameters: {'num_layers': 1, 'hidden_size': 512, 'learning_rate': 0.006159383370476402, 'batch_size': 64}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 15:14:03,974] Trial 16 finished with value: 0.9880112837256286 and parameters: {'num_layers': 1, 'hidden_size': 390, 'learning_rate': 0.0004128702237627426, 'batch_size': 64}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 15:14:44,168] Trial 17 finished with value: 0.9858767073298284 and parameters: {'num_layers': 1, 'hidden_size': 68, 'learning_rate': 0.0031110219107517724, 'batch_size': 64}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 15:16:25,448] Trial 18 finished with value: 0.9890107666286946 and parameters: {'num_layers': 1, 'hidden_size': 331, 'learning_rate': 0.0001650731966347347, 'batch_size': 64}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 15:18:46,010] Trial 19 finished with value: 0.9847603522393644 and parameters: {'num_layers': 2, 'hidden_size': 148, 'learning_rate': 0.0013272198417483693, 'batch_size': 32}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 15:20:10,243] Trial 20 finished with value: 0.9847500853170906 and parameters: {'num_layers': 1, 'hidden_size': 261, 'learning_rate': 0.009951848318855075, 'batch_size': 64}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 15:21:51,423] Trial 21 finished with value: 0.9890072413619018 and parameters: {'num_layers': 1, 'hidden_size': 328, 'learning_rate': 0.0001695329405417513, 'batch_size': 64}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 15:23:44,913] Trial 22 finished with value: 0.9880159633353145 and parameters: {'num_layers': 1, 'hidden_size': 372, 'learning_rate': 0.00015462972621082605, 'batch_size': 64}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 15:25:08,948] Trial 23 finished with value: 0.9880123253216292 and parameters: {'num_layers': 1, 'hidden_size': 316, 'learning_rate': 0.0003393947874737253, 'batch_size': 64}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 15:27:23,703] Trial 24 finished with value: 0.9887585633358619 and parameters: {'num_layers': 1, 'hidden_size': 403, 'learning_rate': 0.00010361132806433923, 'batch_size': 64}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 15:29:30,374] Trial 25 finished with value: 0.9880085321871606 and parameters: {'num_layers': 2, 'hidden_size': 353, 'learning_rate': 0.00021014355365955045, 'batch_size': 64}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 15:31:28,916] Trial 26 finished with value: 0.9892654049688439 and parameters: {'num_layers': 1, 'hidden_size': 461, 'learning_rate': 0.00045222861285862376, 'batch_size': 64}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 15:35:25,083] Trial 27 finished with value: 0.9863837772235728 and parameters: {'num_layers': 1, 'hidden_size': 465, 'learning_rate': 0.0005979251146768348, 'batch_size': 32}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 15:46:59,520] Trial 28 finished with value: 0.9855102834741943 and parameters: {'num_layers': 2, 'hidden_size': 493, 'learning_rate': 0.0005768039148497738, 'batch_size': 16}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 15:47:57,906] Trial 29 finished with value: 0.9891308308786563 and parameters: {'num_layers': 1, 'hidden_size': 446, 'learning_rate': 0.0003804288549743342, 'batch_size': 128}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 15:49:35,737] Trial 30 finished with value: 0.9895138274950301 and parameters: {'num_layers': 1, 'hidden_size': 394, 'learning_rate': 0.0002766246678011725, 'batch_size': 64}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 15:51:21,789] Trial 31 finished with value: 0.9888817194007309 and parameters: {'num_layers': 1, 'hidden_size': 403, 'learning_rate': 0.00026067419360294153, 'batch_size': 64}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 15:53:14,122] Trial 32 finished with value: 0.9872659745343899 and parameters: {'num_layers': 1, 'hidden_size': 454, 'learning_rate': 0.0004820455510658725, 'batch_size': 64}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 15:55:06,172] Trial 33 finished with value: 0.9878866363382846 and parameters: {'num_layers': 1, 'hidden_size': 377, 'learning_rate': 0.00013990444348210374, 'batch_size': 64}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 15:58:59,903] Trial 34 finished with value: 0.9865105070192384 and parameters: {'num_layers': 1, 'hidden_size': 431, 'learning_rate': 0.0008708668647281209, 'batch_size': 64}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 16:18:18,419] Trial 35 finished with value: 0.9852701618046288 and parameters: {'num_layers': 2, 'hidden_size': 480, 'learning_rate': 0.00020804411462151465, 'batch_size': 16}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 16:18:58,302] Trial 36 finished with value: 0.9888820000132735 and parameters: {'num_layers': 1, 'hidden_size': 248, 'learning_rate': 0.0003106473985306064, 'batch_size': 128}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 16:21:41,585] Trial 37 finished with value: 0.9871328748241567 and parameters: {'num_layers': 2, 'hidden_size': 420, 'learning_rate': 0.0004503259456729305, 'batch_size': 64}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 16:23:24,947] Trial 38 finished with value: 0.9893861112497154 and parameters: {'num_layers': 3, 'hidden_size': 306, 'learning_rate': 0.00012416707218982913, 'batch_size': 64}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 16:24:26,462] Trial 39 finished with value: 0.9871379730807698 and parameters: {'num_layers': 3, 'hidden_size': 300, 'learning_rate': 0.0001321818886949256, 'batch_size': 128}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 16:34:58,191] Trial 40 finished with value: 0.9866386473258254 and parameters: {'num_layers': 3, 'hidden_size': 207, 'learning_rate': 0.00012906770633234566, 'batch_size': 16}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 16:37:46,343] Trial 41 finished with value: 0.9872646018835761 and parameters: {'num_layers': 3, 'hidden_size': 357, 'learning_rate': 0.0002482462529786809, 'batch_size': 64}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 16:39:13,172] Trial 42 finished with value: 0.9888873462450686 and parameters: {'num_layers': 2, 'hidden_size': 299, 'learning_rate': 0.0001914942692582179, 'batch_size': 64}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 16:42:38,119] Trial 43 finished with value: 0.9876416515331092 and parameters: {'num_layers': 3, 'hidden_size': 398, 'learning_rate': 0.0002999270399227353, 'batch_size': 64}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 16:43:58,370] Trial 44 finished with value: 0.9875227421105353 and parameters: {'num_layers': 2, 'hidden_size': 241, 'learning_rate': 0.00011880720510158847, 'batch_size': 64}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 16:47:52,943] Trial 45 finished with value: 0.985999442084885 and parameters: {'num_layers': 1, 'hidden_size': 350, 'learning_rate': 0.0011427383761526183, 'batch_size': 32}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 16:50:06,318] Trial 46 finished with value: 0.9880128668768096 and parameters: {'num_layers': 1, 'hidden_size': 434, 'learning_rate': 0.00015126584378282576, 'batch_size': 64}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 16:52:03,932] Trial 47 finished with value: 0.9821299340305861 and parameters: {'num_layers': 2, 'hidden_size': 314, 'learning_rate': 0.0007177175169958739, 'batch_size': 64}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 16:53:47,031] Trial 48 finished with value: 0.9887636223247214 and parameters: {'num_layers': 1, 'hidden_size': 378, 'learning_rate': 0.0002340386041454845, 'batch_size': 64}. Best is trial 12 with value: 0.9900113179487751.\n",
      "[I 2024-12-25 16:54:30,803] Trial 49 finished with value: 0.9871356354441186 and parameters: {'num_layers': 3, 'hidden_size': 183, 'learning_rate': 0.00011060091056601186, 'batch_size': 128}. Best is trial 12 with value: 0.9900113179487751.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'num_layers': 1, 'hidden_size': 349, 'learning_rate': 0.00013466727321237925, 'batch_size': 64}\n"
     ]
    }
   ],
   "source": [
    "# Extract features and labels\n",
    "X_train = np.hstack([\n",
    "    np.stack(final_train_df['resnet_feature']),\n",
    "    np.stack(final_train_df['vit_feature']),\n",
    "    np.stack(final_train_df['clip_feature']),\n",
    "    np.stack(final_train_df['dino_feature'])\n",
    "])\n",
    "y_train = final_train_df['label'].values\n",
    "\n",
    "X_test = np.hstack([\n",
    "    np.stack(valtest_df['resnet_feature']),\n",
    "    np.stack(valtest_df['vit_feature']),\n",
    "    np.stack(valtest_df['clip_feature']),\n",
    "    np.stack(valtest_df['dino_feature'])\n",
    "])\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=1, stratify=y_train\n",
    ")\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    hidden_layer_sizes = tuple([trial.suggest_int('hidden_size', 64, 512) for _ in range(trial.suggest_int('num_layers', 1, 3))])\n",
    "    learning_rate_init = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=hidden_layer_sizes,\n",
    "        learning_rate_init=learning_rate_init,\n",
    "        batch_size=batch_size,\n",
    "        random_state=1\n",
    "    )\n",
    "\n",
    "    mlp.fit(X_train_split, y_train_split)\n",
    "    y_val_pred = mlp.predict(X_val_split)\n",
    "    f1 = f1_score(y_val_split, y_val_pred, average='macro')\n",
    "    return f1\n",
    "# Perform Optuna optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5e9ef4b-4138-4051-8caa-b7477370f8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MLPClassifier with best parameters from Optuna\n",
    "best_params = study.best_params\n",
    "hidden_layer_sizes = tuple([best_params['hidden_size']] * best_params['num_layers'])\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=hidden_layer_sizes,\n",
    "    learning_rate_init=best_params['learning_rate'],\n",
    "    batch_size=best_params['batch_size'],\n",
    "    random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78d8356e-7054-4b57-81f2-c2ae03c2f24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score across 5 folds: 0.9891\n"
     ]
    }
   ],
   "source": [
    "# 5-Fold Cross-Validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "f1_scores = []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_train, y_train):\n",
    "    X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=hidden_layer_sizes,\n",
    "        learning_rate_init=best_params['learning_rate'],\n",
    "        batch_size=best_params['batch_size'],\n",
    "        random_state=1\n",
    "    )\n",
    "\n",
    "    mlp.fit(X_train_fold, y_train_fold)\n",
    "    y_val_pred = mlp.predict(X_val_fold)\n",
    "    f1 = f1_score(y_val_fold, y_val_pred, average='macro')\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "print(f\"Average F1 Score across 5 folds: {np.mean(f1_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe75c8e9-e7a9-4a48-b25f-f19303a501f5",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron is the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce420ec6-e06c-4047-94b1-0752c9141388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on whole set\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = mlp.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
